<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Generating Multimodal Driving Scenes via Next-Scene Prediction">
  <meta name="keywords" content="DriveDreamer">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Generating Multimodal Driving Scenes via Next-Scene Prediction</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/ms_icon.png">

  <style>  
    table {  
      font-family: arial, sans-serif;  
      border-collapse: collapse;  
      width: 100%;  
    }  
      
    td, th {  
      border: 2px solid #F1F4F5;  
      text-align: left;  
      padding: 8px;  
    }  
    tr:nth-child(3n - 1) {  
      background-color: #F1F4F5;  
    }  

    tr:nth-child(3n) {  
      border: 2px solid #FFFFFF;
    }  
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
<!--<nav class="navbar" role="navigation" aria-label="main navigation">-->
<!--  <div class="navbar-brand">-->
<!--    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--    </a>-->
<!--  </div>-->
<!--  <div class="navbar-menu">-->
<!--    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">-->
<!--      <a class="navbar-item" href="https://yanhaowu.github.io/">-->
<!--      <span class="icon">-->
<!--          <i class="fas fa-home"></i>-->
<!--      </span>-->
<!--      </a>-->


<!--    </div>-->

<!--  </div>-->
<!--</nav>-->





<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Generating Multimodal Driving Scenes via Next-Scene Prediction</h1>
          <h2 class="is-size-2">CVPR 2025</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://yanhaowu.github.io/">Yanhao Wu</a><sup>1, 2</sup>,
            </span>
              <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=PlMpgeIAAAAJ&hl=zh-CN&oi=ao">Haoyang Zhang</a><sup>2</sup>,
            </span>
              <span class="author-block">
              <a href="https://wzmsltw.github.io/">Tianwei Lin</a><sup>2</sup>,
            </span>
              <span class="author-block">
              <a href="https://www.linkedin.com/in/alanhuang1990/">Lichao Huang</a><sup>2</sup>,
            </span>
               <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=Z_ZkkbEAAAAJ&hl=zh-CN&oi=ao/">Rui Wu</a><sup>2</sup>,
            </span>
              <span class="author-block">
              <a href="https://congpeiqiu.github.io/">Congpei Qiu</a><sup>1</sup>,
            </span>
            <span class="author-block">
                <a href="https://gr.xjtu.edu.cn/en/web/wei.ke/home/">Wei Ke</a><sup>1</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://people.epfl.ch/tong.zhang?lang=en">Tong Zhang</a><sup>3</sup>,
            </span>

          </div>


          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Xi'an Jiaotong University</span>&nbsp;&nbsp;
            <span class="author-block"><sup>2</sup>Horizon Roboics</span>&nbsp;&nbsp;
            <span class="author-block"><sup>3</sup>EPFL</span>&nbsp;&nbsp;
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2303.16235.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

<!--              <span class="link-block">-->
<!--                <a href="https://arxiv.org/pdf/2303.16235.pdf"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="ai ai-arxiv"></i>-->
<!--                  </span>-->
<!--                  <span>arXiv</span>-->
<!--                </a>-->
<!--              </span>-->

              <!-- Code Link. -->
<!--              <span class="link-block">-->
<!--                <a href="https://github.com/YanhaoWu/STSSL/"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-github"></i>-->
<!--                  </span>-->
<!--                  <span>Code</span>-->
<!--                  </a>-->
<!--              </span>-->




            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- <div class="hero-body"> -->
    <div class="item">
      <img src="static/Teaser.png" alt="MY ALT TEXT" />
      <h2 style="font-size: 18px" class="subtitle has-text-centered">
        <b>Functions of our UMGen.
          <!-- <br> -->
        </b> (a) Starting from a random initialization, UMGen generates ego-centric, multimodal scenes frame-by-frame. Each scene encompasses four modalities: ego-vehicle action, map, traffic
agent, and image. (b) UMGen offers multiple functions. It can not only imagine multimodal scene sequences freely but can also predict
the other multimodalities based on input ego-vehicle actions. Furthermore, UMGen can incorporate user-specified agent actions to create
customized scene sequences.</h2>
    </div>
  </div>
</section>


  <section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
<!--    <div class="columns is-centered has-text-centered">-->
      <div class="column is-full_width">
<!--        <img src="static/Teaser.png" class="center"/>-->
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Generative models in Autonomous Driving (AD) enable diverse scenario creation,
            yet existing methods fall short by only capturing a limited range of modalities,
            restricting the capability of generating controllable scenarios for comprehensive evaluation of AD systems.
            In this paper, we introduce a multimodal generation framework that incorporates four major data modalities, including a novel addition of map modality.
            With tokenized modalities, our scene sequence generation framework autoregressively predicts
            each scene while managing computational demands through a two-stage approach.
            The Temporal AutoRegressive (TAR) component captures inter-frame dynamics for each modality while the Ordered AutoRegressive (OAR) component
            aligns modalities within each scene by sequentially predicting tokens in a fixed order. To maintain coherence between map and ego-action modalities,
            we introduce the Action-aware Map Alignment (AMA) module, which applies a transformation based on the ego-action to maintain coherence between these modalities.
            Our framework effectively generates complex, realistic driving scenarios over extended sequences,
            ensuring multimodal consistency and offering fine-grained control over scenario elements.
        </div>
<!--      </div>-->
    </div>

  </div>
</section>

<style>
  .hero {
    margin-left: 20px; /* 添加左边距 */
  }
</style>
<!-- Image carousel -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- <div class="hero-body"> -->
    <h2 class="title">Method</h2>
    <div class="item">
      <img src="static/method.png" alt="MY ALT TEXT" />
      <h2 style="font-size: 18px" >
        <b>Pipeline of our UMGen. </b>
        Given T past frames of multimodal driving scenes,
        including ego-action, maps, traffic agents, and images at each scene, each modality is tokenized into discrete tokens.
        The token embeddings are then processed through the Ego-action Prediction module, which forecasts the ego-action
        for T+1 time step. Using this predicted ego-action, the AMA module adjusts the map features.
        Finally, the TAR module aggregates temporal information across sequences, while the OAR module ensures sequential
        prediction within each frame by autoregressively generating each token conditioned on the aggregated history information.
        The predicted tokens are fed to the decoder to obtain the next scene.</h2>
    </div>
  </div>
</section>
<!-- End image carousel -->



<section class="section" id="Results" style="margin-top: -10px;"> <!-- 使用负 margin -->
    <div class="container is-max-desktop content">
    <section class="hero method">
      <h2 class="title">Visualization</h2>

<!--      <div class="container is-max-desktop">-->
<!--        <div class="hero-body" style="font-size: 1.3em; line-height: 1.4;">-->
<!--          <p style="text-indent: 7px; margin-bottom: 10px;">-->
<!--            UMGen offers multiple functions:-->
<!--          </p>-->
<!--          <ul style="padding-left: 30px; list-style-type: disc; margin-bottom: 10px;">-->
<!--            <li>-->
<!--              It can imagine multimodal driving scene sequences freely.-->
<!--            </li>-->
<!--            <li>-->
<!--              It can predict other multimodalities based on input ego-vehicle actions.-->
<!--            </li>-->
<!--            <li>-->
<!--              It can incorporate user-specified agent actions to create customized scene sequences.-->
<!--            </li>-->
<!--          </ul>-->
<!--          <p style="text-indent: 0px; margin-bottom: 10px;">-->
<!--            We provide visualizations and a video demonstration showcasing UMGen's autoregressive scenario generation capabilities.-->
<!--          </p>-->
<!--        </div>-->
<!--      </div>-->
    </section>
  </div>

  <div class="container is-max-desktop content">
    <h3 class="title" style="margin-bottom: 20px;">1. Driving Scenario Generation</h3>
    <div>
      <h4 class="subtitle" style="margin-bottom: 10px;">A. Long-duration Driving Scenario Generation</h4>
      <section class="hero method">
        <div class="container is-max-desktop">
          <div class="hero-body" style="padding-top: 0;">
            <video preload="auto" id="tree-long" autoplay controls muted loop style="width: 100%; max-width: 1600px;">
              <source src="./static/videos_example/1_Generated_Scenes/A_Generated_Long_Scenes/meraged_videos.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </section>
    </div>

    <div>
      <h4 class="subtitle" style="margin-bottom: 10px;">B. Diverse Driving Scenario Generation</h4>
      <section class="hero method">
        <div class="container is-max-desktop">
          <div class="hero-body" style="padding-top: 0;">
            <video preload="auto" id="tree-diverse" autoplay controls muted loop style="width: 100%; max-width: 1600px;">
              <source src="./static/videos_example/1_Generated_Scenes/B_Generated_Diverse_Scenes/Diverse_video.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </section>
    </div>
  </div>
</section>
  




  <section class="section" id="Results2" style="margin-top: 0;">
    <div class="container is-max-desktop content">
      <h3 class="title" style="margin-bottom: 10px;">2. Interactive Ego-vehicle Control</h3>
      <section class="hero method">
        <div class="container is-max-desktop">
          <div class="hero-body" style="padding-top: 0;"> 
            <h4 class="subtitle" style="margin-top: 0px; margin-bottom: 10px;">A. We control the ego-vehicle to either drive straight or make a right turn at the intersection.</h4>
          <div class="container is-max-desktop" style="display: flex; justify-content: center;"> 
            <video preload="auto" poster="" id="tree" autoplay controls muted loop style="width: 60%; max-width: 800px; outline: 5px;"> 
              <source src=".\static\videos_example\2_Ego_Control\A_Intersection\combined_video.mp4" type="video/mp4">
            </video>
          </div> 

          <h4 class="title" style="margin-top: 70px;">B. We control the ego-vehicle to either wait behind the agent or execute a lane change to overtake.</h4>
          <div class="container is-max-desktop" style="display: flex; justify-content: center;"> 
            <video preload="auto" poster="" id="tree" autoplay controls muted loop style="width: 60%; max-width: 800px; outline: 5px;"> 
              <source src=".\static\videos_example\2_Ego_Control\B_Waiting\combined_video.mp4" type="video/mp4">
            </video>
          </div>
          
        </div>
      </div>
    </section>
  </div>
</section>


<section class="section" id="Results3" style="margin-top: 0;">
  <div class="container is-max-desktop content">
    <h3 class="title" style="margin-bottom: 15px;">3. User-Specified Scenario Generation</h3>
    
    <section class="hero method">
      <h4 class="title" style="margin-top: 7px;"> We can control the agent to simulate a cut-in maneuver scenario. The ego-vehicle brakes or executes a controlled lane change to avoid a collision.</h4>
      <div class="container is-max-desktop">
        <div class="hero-body" style="padding-top: 0;">  
          <div class="container is-max-desktop">
            <video preload="auto"poster="" id="tree" autoplay controls muted loop width="12000px" outline="0px"> 
              <source src=".\static\videos_example\3_User_Specified_Scenario_Generation\Userset_Scene.mp4"
              type="video/mp4">
            </video>
        </div> 
        </div>
      </div>
    </section>
  </div>
</section>




<section class="section" id="Results4" style="margin-top: 0;">
  <div class="container is-max-desktop content">
    <h3 class="title" style="margin-bottom: 15px;">4. Demonstration of Autoregressive Scenario Generation by UMGen</h3>
    <section class="hero method">
      <h4 class="title" style="margin-top: 7px;">  UMGen autoregressively generates ego-centric, multimodal scenes frame-by-frame </h4>
      <div class="container is-max-desktop">
        <div class="hero-body" style="padding-top: 0; display: flex; justify-content: center;">  
          <div class="container is-max-desktop" style="text-align: center;">
            <video preload="auto" poster="" id="tree" autoplay controls muted loop style="width: 60%; max-width: 800px; outline: 5px;"> 
              <source src=".\static\videos_example\4_Example_AR\AR_example.mp4" type="video/mp4">
            </video>
          </div> 
        </div>
      </div>
    </section>
  </div>
</section>

<section class="section" id="Results5" style="margin-top: 0;">
  <div class="container is-max-desktop content">
    <h3 class="title" style="margin-bottom: 15px;">5. Integrating a Diffusion Decoder</h3>
    <section class="hero method">
      <h4 class="title" style="margin-top: 7px;">
        A transformer-based diffusion decoder can be integrated to generate high-quality videos using the output of UMGen as conditions.
      </h4>
      <div class="container is-max-desktop">
        <div class="hero-body" style="padding-top: 0; display: flex; justify-content: center; width: 100%;">
          <div class="container is-max-desktop" style="text-align: center; width: 100%;">
            <video preload="auto" poster="" id="tree" autoplay controls muted loop style="width: 100%; max-width: none; outline: none;">
              <source src=".\static\videos_example\5_Diffusion\compare_diffusion_converted.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </section>
  </div>
</section>




  </div></div></section>
  </div>
</section>


</body>
</html>
